{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL31h2jWlgVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def scan_vocabulary(sents, tokenize, min_count=2):\n",
        "  counter = Counter(w for sent in sents for w in tokenize(sent))\n",
        "  counter = {w:c for w,c in counter.items() if c >= min_count}\n",
        "  idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
        "  vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
        "  return idx_to_vocab, vocab_to_idx"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn-F06dJmhhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def coocurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):\n",
        "  counter = defaultdict(int)\n",
        "  for s, tokens_i in enumerate(tokens):\n",
        "    vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
        "    n = len(vocabs)\n",
        "    for i, v in enumerate(vocabs):\n",
        "      if windows <= 0:\n",
        "        b, e, = 0, n\n",
        "      else:\n",
        "        b = max(0, i-window)\n",
        "        e = min(i+window, n)\n",
        "      for j in range(b, e):\n",
        "        if i == j:\n",
        "          continue\n",
        "        counter[(v, vocab[j])] += 1\n",
        "        counter[(vocabs[j], v)] += 1\n",
        "    counter= {k:v for k,v in counter.items() if v>= min_coocurrence}\n",
        "    n_vocabs = len(vocab_to_idx)\n",
        "    return dict_to_mat(counter, n_vocabs, n_vocabs)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCFv5Wozn1n0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def dict_to_mat(d, n_rows, n_cols):\n",
        "  rows, cols, date = [], [], []\n",
        "  for (i, j), v in d.items():\n",
        "    rows.append(i)\n",
        "    cols.append(j)\n",
        "    data.append(v)\n",
        "  return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQeHG4GeocfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_graph(sents, tokenize=None, min_count=2, window=2, min_coocurrence=2):\n",
        "  idx_to_vocab , vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "  tokens = [tokenize(sent) for sent in sents]\n",
        "  g = cooccurrence(tokens, vocab_to_idx, window, min_coocurrence)\n",
        "  return g, idx_to_vocab"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UONxe2VHo8sJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 스팸 메일 데이터 로딩\n",
        "# tokenize는 함수로 하나 만들어서 nltk 활용해서 만들기\n",
        "def tokenize(sentence):\n",
        "  # sentence tokenizing\n",
        "  # word? lemma? stem? tokenizing\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}