{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리(영어)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소문자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts=[\"South Korea\",\"south korea\",\"South Korea\",\"South korea\"]\n",
    "lower_words=[word.lower() for word in texts]\n",
    "lower_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 어간 추출기 생성\n",
    "porter_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 예시 단어 connect \n",
    "words=[\"connect\",\"connected\",\"connection\",\"connections\",\"connects\"]\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "\n",
    "stemdf= pd.DataFrame({'original_word': words,'stemmed_word': stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 단어 trouble\n",
    "words=[\"trouble\",\"troubled\",\"troubles\",\"troublemsome\"]\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "\n",
    "stemdf= pd.DataFrame({'original_word': words,'stemmed_word': stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표제어 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 표제어 복원기 생성\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trouble 변이단어 표제어 복원\n",
    "words=[\"trouble\",\"troubling\",\"troubled\",\"troubles\",\"troublesome\",\"run\",\"ran\",\"runing\",]\n",
    "lemmatized_words=[lemmatizer.lemmatize(word=word,pos='v') for word in words]\n",
    "lemmatizeddf= pd.DataFrame({'original_word': words,'lemmatized_word': lemmatized_words})\n",
    "lemmatizeddf=lemmatizeddf[['original_word','lemmatized_word']]\n",
    "lemmatizeddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goose 표제어 복원 예시\n",
    "words=[\"goose\",\"geese\"]\n",
    "lemmatized_words=[lemmatizer.lemmatize(word=word,pos='n') for word in words]\n",
    "lemmatizeddf= pd.DataFrame({'original_word': words,'lemmatized_word': lemmatized_words})\n",
    "lemmatizeddf=lemmatizeddf[['original_word','lemmatized_word']]\n",
    "lemmatizeddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['this','that','and','a','we','it','to','is','of','up','need'] # 이때 중요한 정보가 있을 수 있으니, 주의해야 한다.\n",
    "text=\"this is a text full of content and we need to clean it up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=text.split(\" \")\n",
    "shortlisted_words=[]\n",
    "\n",
    "# 불용어 제거\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        shortlisted_words.append(w)\n",
    "    else:\n",
    "        shortlisted_words.append(\"W\")\n",
    "\n",
    "print(\"original sentence = \",text)    \n",
    "print(\"sentence with stop words removed= \",' '.join(shortlisted_words))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노이즈 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노이즈 섞인 단어 처리\n",
    "raw_words=[\"..trouble..\",\"trouble<\",\"trouble!\",\"<a>trouble</a>\",'1.trouble']\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in raw_words]\n",
    "stemdf= pd.DataFrame({'raw_word': raw_words,'stemmed_word': stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "    # HTML 마크업 심볼 제거 \n",
    "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    # 단어 이외의 심볼 제거\n",
    "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    \n",
    "    # 공백 문자 제거\n",
    "    text=text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노이즈 제거된 단어를 추가적으로 stemming\n",
    "cleaned_words=[scrub_words(w) for w in raw_words]\n",
    "cleaned_stemmed_words=[porter_stemmer.stem(word=word) for word in cleaned_words]\n",
    "stemdf= pd.DataFrame({'raw_word': raw_words,'cleaned_word':cleaned_words,'stemmed_word': cleaned_stemmed_words})\n",
    "stemdf=stemdf[['raw_word','cleaned_word','stemmed_word']]\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
