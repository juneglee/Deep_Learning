# 활성화 함수 계층 구현하기 (ReLU, Sigmoid)
# 
# sigmoid 계층 

# 1단계 
# '/' 노드, 즉 y=1/x을 미분하면 -y^2가 된다
# 역잔파 떄는 상류에서 흘러온 값에 -y^2 (순전파의 출력을 제곱한 후 마이너스를 붙인 값)을
# 곱해서 하류로 전달합니다. 
# 2단계
# '+' 노드, 상류의 값을 여과 없이 하류로 보낸다 
# 3단계 
# 'exp'노드 y = exp(x) 연ㅅㄴ을 수행
# 상류의 값에 순전파 때의 출력 (exp(-x)) 을 곱해 아류로 전파 
# 4단계 
# 'x' 노드는 순전파 때의 값을 서로 바꿔 곱한다 
# common/layers.py 참조 def sigmoid
