# 더 빠르게 (딥러닝 고속화)
# CPU만으로 딥러닝을 처리하기는 부족한게 현실이며,
# 딥러닝 프레임워크 GPU(Graphics Processing Unit)를 활용해 대량의 연산을 고속으로 처리할 수 있다
# 또한 GPU와 여러 기기로 분산 수행하기도 한다 

# GPU는 대량의 병렬 연산에 특화 되어있으며, CPU는 연속적인 복잡한 계산을 잘 처리한다 
# GPU는 엔비디아와 AMD에서 제공하며
# GPU는 법용 수치 연산에 이용하고, 엔비디아를 많이 사용한다 
# 엔비디아의 GPU 컴퓨팅용 통합 개발 환경인 CUDA를 사용하기 때문이다 
# cuDNN은 CUDA 위에서 동작하는 라이브러리로, 딥러닝 최저고하된 함수 등이 구현되어 있다 

# 합성곱 계층에서 행하는 연산은 im2col을 이용해 큰 행렬의 내적으로 변환할 수 있다 
# 이러한 im2col의 방식은 GPU로 구현하기에도 적합하다. GPU는 작은 단위로 계산하기 보다는 큰 덩어리를 한번에 계산하는데 유리하기 떄문이다. 
# 즉, im2col로 거대한 행렬의 내적으로 한번에 계산하며 GPU의 잠재력을 끌어내는 것이다 

# 분산학습
# 딥러닝 학습에서는 수평확장 (scale out)하자는 아이디어(즉, 분산학습)가 중요하다 
# 이러한 프레임워크들이 나타나고 있으며,
# 구굴의 텐서플로와 마이크로소프트의 CNTK(Computational Network ToolKit)는 분산학습에 역점을 두고 있다 
# 거대한 데이터센터의 저지연, 고처리량 네트워크 위에서 이들의 프레임 워크가 수행하는 분사 학습은 놀라운 효과를 보이고 있다 

# 연산 정밀도와 비트 줄이기 
# 계산 능력 외에도 메모리 용량과 버스 대역폭 등이 딥러닝 고속화에 병목이 될 수 있다
# 메모리 용량 면에서는 대량의 가중치 매개변수와 중간 데이터를 메모리에 저장해야 한다는 것을 생각해야 한다 
# 
# 컴퓨터는 주로 64비트나 32비트 부동소수점 수를 사용해 실수를 표현한다
# 많은 비트를 사용할수록 계산 오차는 줄어듭니다만, 그 만큼 계산에 드는 비용과 메모리 사용량이 늘고 버스 대역폭에 부담을 준다 
# 다행히 딥러닝은 높은 수치 정밀도(수치를 몇 비트로 표현하느냐)를 요구하지 않지만, 이는 신겸앙의 중요한 성질 중 하나로, 신경마의 견고성에 따른 특성이다 
# 
# 컴퓨터에서 실수를 표현하는 방식으로 32비트 단정밀도 (single-precision)와 64비트 배정밀도(double-precision) 부동소수점 등의 포맷이 있다 
# 지금까지의 실험으로는 딥러닝 16비트 반정밀도(half-precision) 만 사용해도 문제가 없다 
# 실제로 엔비디아의 GPU 인 파스칼(Pascal) 아키덱처는 이 포맷을 지원하여, 반정밀도 부동소수점이 표준적으로 이용되리라 생각한다 