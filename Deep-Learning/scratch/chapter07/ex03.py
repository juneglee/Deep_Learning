# 딥러닝의 초기 역사 
# 딥러닝이 주목을 받게 된계기는 이미지 인식 기술을 겨루는 장인 ILSVRC의 대회이다 
# 일명 AlexNet이 압도적인 성적으로 우숭하면서 그 동안의 이미지 인식에 대한 접근법의 뿌리부터 뒤흔들었다 
# 이후 150층이 넘는 심층 신경망인 ResNet이 오류율 3.5%까지 낮췄다
# 최근에는 VGG, GoogleNet, ResNet은 특히 유명하며, 다양한 딥러닝 분야에서 사용중이다 

# 이미지 넷(imageNet)
# 100만 장이 넘는 이미지를 담고 있는 데이터 셋 

# VGG
# 합성곱 계층가 풀링 계층으로 구성되는 기본적인 CNN이다 
# 비중 있는 층(합성곱 계층, 완전연결 계층)을 모두 16층(혹은 19층)으로 심화한게 특징이다 
# 층의 깊이에 따라 VGG16, VGG19로 구분한다 
# 3x3 의 작은 필터를 사용한 합성곱 계층을 연속으로 거친다는 것
# 합성곱 계층을 2~4회 연속으로 풀링 계층을 두어 크기를 절반으로 줄이는 처리를 반복한다 
# 마지막으에는 완전연결 계층을 통과시켜 결과를 출력

# GoogleNet
# 사각형이 합성곱 계층과 풀링 계층 등의 계층으로 나타난대 
# GoogLeNet은 세로 뱡향 깊이 뿐 아니라 가로 방향도 깊다는 점이 특징이다 
# 가로 방향의 폭은 인셉션 구조라 한다 
# 인셉션 구조는 크기가 다른 필터(와 풀링)을 여러 개 적용하여 그 결과를 결합한다 
# 인셉션 구조를 하나의 빌딩 블록(구성요소)으로 사용하는 것이 GoogLeNet의 특징이다 
# 1x1 크기의 필터를 사용한 합성곱 계층을 많은 곳에서 사용하며, 1x1의 합성곱 연산은 채널쪽으로 크기를 줄이는 것으로,
# 매개변수 제거와 고속 처리에 기여한다 

# ResNet
# 마이크로소프트의 팀이 개발한 네트워크이다 
# 딥러닝 학습에서는 층이 지나치게 깊으면 학습이 잘 되지 않고, 오히려 성능이 떨어지는 경우가 많다
# ResNet은 이러한 문제를 해결하기 위해서 스킵연결(skip connection)을 도입
# 이 구조가 층의 깊이에 비례해 성능을 향상키실 수 있게 한 핵심이다(물론 층을 깊게하는데 여전히 한계)

# 스킵연결: 입력데이터를 합성곱 계층을 건너뛰어 출력에 바로 더하는 구조 
# 입력데이터를 그대로 흘리는 것으로, 역전파 때도 상류의 기울기르 ㄹ그대로 하루로 보낸다 
# 여기서 핵심은 상류의 기울기에 아무런 수정을 가하지 안혹 그대로 흘린다는 것이다 
# 그래서 스킵 연겨롤 기울기가 작아지거나 지나치게 커질 걱정 없이 앞 층에 의미 있는 기울기가 전해지지라 기대할 수 있다 
# 층을 깊게 할 수록 기울기가 작아지는 소실 문제를 이 스킵 연결이 줄여주는 것이다 

# 이미지넷이 제공하는 거대한 데이터셋으로 학습한 가중치 값들은 실제 제품에 활용해도 효과적이고, 
# 또 많이 그렇게 이용하고 있다. 이를 전이학습(transfer learning)이라고 해서, 학습된 가중치(혹은 그 일부)를
# 다른 신경망에 복사한 다음, 그 상태로 재학습을 수행한다
# 예를 들어 VGG와 같은 신견망을 준비하고, 미리 학습된 가중치를 초깃값으로 설정한 후,
#  새로운 데이터셋을 대상으로 재학습(fine tuning)을 수행합니다
# 전이 학습은 보유한 데이터셋이 적을 때 특히 유용한 방법이다 

# 더 빠르게 (딥러닝 고속화)
# CPU만으로 딥러닝을 처리하기는 부족한게 현실이며,
# 딥러닝 프레임워크 GPU(Graphics Processing Unit)를 활용해 대량의 연산을 고속으로 처리할 수 있다
# 또한 GPU와 여러 기기로 분산 수행하기도 한다 

# GPU는 대량의 병렬 연산에 특화 되어있으며, CPU는 연속적인 복잡한 계산을 잘 처리한다 
# GPU는 엔비디아와 AMD에서 제공하며
# GPU는 법용 수치 연산에 이용하고, 엔비디아를 많이 사용한다 
# 엔비디아의 GPU 컴퓨팅용 통합 개발 환경인 CUDA를 사용하기 때문이다 
# cuDNN은 CUDA 위에서 동작하는 라이브러리로, 딥러닝 최저고하된 함수 등이 구현되어 있다 

# 합성곱 계층에서 행하는 연산은 im2col을 이용해 큰 행렬의 내적으로 변환할 수 있다 
# 이러한 im2col의 방식은 GPU로 구현하기에도 적합하다. GPU는 작은 단위로 계산하기 보다는 큰 덩어리를 한번에 계산하는데 유리하기 떄문이다. 
# 즉, im2col로 거대한 행렬의 내적으로 한번에 계산하며 GPU의 잠재력을 끌어내는 것이다 